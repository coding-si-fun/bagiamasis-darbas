{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Projekto aprašymas\n"
      ],
      "metadata": {
        "id": "57bBWj0qoj-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tikslas: apmokyti google/gemma-2-2b-it modulį, kad imituotu mano rašymo stilių."
      ],
      "metadata": {
        "id": "KLqWe2M8o18_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duomenis gavvau parsisiuntęs iš facebook savo asmenines žinutes, todėl turiniu nesidalinsiu, dėl privatumo.\n",
        "Bet notebook failuose matyti kaip duomenys yra apdorojami."
      ],
      "metadata": {
        "id": "EBkVAkURpKm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "facebook pateikia žinutes kiekvienam asmeniui, paskiriant po failą \"message_1.json\". kuris randasi tokioje direktorijoje. Your_facebook_acitivity -> messages ->e2ee_cutover->aidasaidas_1517630632173269 -> message_1.json"
      ],
      "metadata": {
        "id": "aJKR6_XipV9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaip toliau duomenys yra apdorojami matosi notebook."
      ],
      "metadata": {
        "id": "_NrvqRKrpavq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasirinkau google/gemma-2-2b-it modulį, kad mažesnį, nes pabandžius didesnius labai daug reikalauja resursų, tai nusprenžiau nuomoti iš google colab."
      ],
      "metadata": {
        "id": "kKR7wH10pc5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neleabai sekėsi iš pradžių apmokyti, nes nuolat nutrūkdavo mokymo procesas, nes trūo atiminties GPU, tad kol išmokau kvantizuoti, praėjo nemažai laiko. Bet galiausiai susitvarkiau.\n",
        "\n",
        "Kita problema buvo, kad apmokius su turimais duomenimis, modelis nelabai atsakinėdavo į klausimus taip, kaip reikėtų. Mokiau visą modulį, todėl su mažesniu duomenų kiekiu rezultatai buvo patenkinami, tačiau panaudojus visas 7200 klausimo-atsakymo poras, modelis tiesiog atsakinėdavo tuo pačiu klausimu. Tuomet sužinojau apie LoRa.\n",
        "\n",
        "Lorą pritaikiau, nors iš pradžių sekėsi nelabai sėkmingai, nes buvo įvairių dokumentacijų, kaip tai padaryti:\n",
        "\n",
        "Šaldyti paskutinį sluoksnį ir mokyti, tačiau tai nepasiteisino.\n",
        "Pritaikiau paprastai – rezultatai pasirodė geresni, tačiau vis tiek kažko trūko, nes treniravau tik du sluoksnius: \"q_proj\", \"v_proj\".\n",
        "Atradau, kad priderinus LoRa tam tikrus sluoksnius, kurie kartojasi (ką radau internete):\n",
        "target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"].\n",
        "Kadangi sugaišau nemažai laiko, bandydamas rasti tinkamus nustatymus, nusprendžiau parašyti ciklą, kuris galėtų apmokyti 6 modulius su keliais LoRa nustatymais. Klausimus sudėjau į failą pasaulio_žinios.csv, kurį padėjo parengti savanoriai draugai, kad būtų galima įvertinti modelį. Taip pat pasidomėjau, kokiais rodikliais remtis po apmokymo. Tam pasirinkau BLEU, kuris pasirodė visiškai neefektyvus, ir ROUGE, kuris pasiteisino.\n",
        "\n",
        "Paleidau mokymą su keliais skirtingais nustatymais ir nustatiau, kad 4-tas LoRa nustatymas buvo pats tinkamiausias – jis nei underfitino, nei overfitino ant kelių klausimų. Tada apmokiau modelį su šiais nustatymais. Išvados pateiktos aplanke Pradinė_analizė."
      ],
      "metadata": {
        "id": "N6nZBApZpqor"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYRmLBWypsRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}